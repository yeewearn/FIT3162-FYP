{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>review</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Worth Buying For The Pictures Alone (As Ever)</td>\n",
       "      <td>4</td>\n",
       "      <td>6 April 2014</td>\n",
       "      <td>By\\n\\n    \\n\\n    Copnovelist\\n\\n  \\n\\n on 6 ...</td>\n",
       "      <td>Part of the magic for me growing up as a boy ...</td>\n",
       "      <td>Hornby 2014 Catalogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing detail fabulous photography.</td>\n",
       "      <td>5</td>\n",
       "      <td>11 April 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    richard\\n\\n  \\n\\n on 11 Apr...</td>\n",
       "      <td>Amazing detail, every credit to the photograp...</td>\n",
       "      <td>Hornby 2014 Catalogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>'Great Purchase'</td>\n",
       "      <td>5</td>\n",
       "      <td>23 April 2014</td>\n",
       "      <td>By\\n\\n    \\n\\n    Pinkhandbag\\n\\n  \\n\\n on 23...</td>\n",
       "      <td>This was purchased on behalf of my Dad. He is...</td>\n",
       "      <td>Hornby 2014 Catalogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Great Catalogue</td>\n",
       "      <td>5</td>\n",
       "      <td>11 Jun. 2014</td>\n",
       "      <td>By\\n\\n    \\n\\n    Gary John Mapson\\n\\n  \\n\\n ...</td>\n",
       "      <td>Everything I really needed to see what was on...</td>\n",
       "      <td>Hornby 2014 Catalogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I collect them all as the glossy pictures are...</td>\n",
       "      <td>5</td>\n",
       "      <td>7 Dec. 2014</td>\n",
       "      <td>By\\n\\n    \\n\\n    David Baker\\n\\n  \\n\\n on 7 ...</td>\n",
       "      <td>I collect them all as the glossy pictures are...</td>\n",
       "      <td>Hornby 2014 Catalogue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  rating  \\\n",
       "0           0     Worth Buying For The Pictures Alone (As Ever)        4   \n",
       "1           1              Amazing detail fabulous photography.        5   \n",
       "2           2                                  'Great Purchase'        5   \n",
       "3           3                                   Great Catalogue        5   \n",
       "4           4   I collect them all as the glossy pictures are...       5   \n",
       "\n",
       "              date                                             author  \\\n",
       "0    6 April 2014    By\\n\\n    \\n\\n    Copnovelist\\n\\n  \\n\\n on 6 ...   \n",
       "1   11 April 2015    By\\n\\n    \\n\\n    richard\\n\\n  \\n\\n on 11 Apr...   \n",
       "2   23 April 2014    By\\n\\n    \\n\\n    Pinkhandbag\\n\\n  \\n\\n on 23...   \n",
       "3    11 Jun. 2014    By\\n\\n    \\n\\n    Gary John Mapson\\n\\n  \\n\\n ...   \n",
       "4     7 Dec. 2014    By\\n\\n    \\n\\n    David Baker\\n\\n  \\n\\n on 7 ...   \n",
       "\n",
       "                                              review                product  \n",
       "0   Part of the magic for me growing up as a boy ...  Hornby 2014 Catalogue  \n",
       "1   Amazing detail, every credit to the photograp...  Hornby 2014 Catalogue  \n",
       "2   This was purchased on behalf of my Dad. He is...  Hornby 2014 Catalogue  \n",
       "3   Everything I really needed to see what was on...  Hornby 2014 Catalogue  \n",
       "4   I collect them all as the glossy pictures are...  Hornby 2014 Catalogue  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd                            # to analyse data that are stored in a csv file\n",
    "import numpy as np                             # to provide a large set of numeric datatypes that can be used to construct arrays\n",
    "import nltk                                    # a platform for building Python programs to work with human language data\n",
    "import re                                      # regex model\n",
    "import datetime\n",
    "from nltk.corpus import stopwords              # to remove stopwords\n",
    "from nltk.stem import WordNetLemmatizer        # to lemmatize\n",
    "from nltk.corpus import wordnet                # used to check whether the word is an adjective, noun, verb or adverb\n",
    "from sklearn.feature_extraction.text import TfidfTransformer         # to run tfidf transformer on the given data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          # to run tfidf vectorizer on the given data\n",
    "\n",
    "# Read the csv file\n",
    "dat = pd.read_csv('preprocessed_review.csv')\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"\n",
    "    This function gets the wordnet postag of each words.\n",
    "    :param word: word in each review texts\n",
    "    :returns: the postag of each word \n",
    "    Retrieved from: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"R\":wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(review):\n",
    "    \"\"\"\n",
    "    This function takes in a list and preprocess accordingly. \n",
    "    :param review: list as input\n",
    "    :returns: preprocessed words\n",
    "    Retrieved from https://pythonspot.com/nltk-stop-words/\n",
    "    \"\"\"\n",
    "    result = re.sub(r'\\d+','', review)      # Remove numbers/ digits\n",
    "    result = re.sub(r'[^\\w\\s]','',result)   # Remove puntuations\n",
    "    val = result.lower()                    # Convert all the reviews to lowercase\n",
    "    new_list = []\n",
    "    for word in val.split():                # tokenize and stop words removal\n",
    "        if word not in stop_words:\n",
    "            new_list.append(word)\n",
    "    \n",
    "    return new_list\n",
    "    \n",
    "    \n",
    "def lemmatize_it(series_list):\n",
    "    \"\"\"\n",
    "    This function is to carry out lemmatization on the\n",
    "    tokenized review\n",
    "    :series_list: a list that contains token to be lemmatized\n",
    "    :returns: lemmatized word\n",
    "    Retrieved from: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \"\"\"\n",
    "    stem_it = []\n",
    "    for i in series_list:\n",
    "        lem = lemmatizer.lemmatize(i, get_wordnet_pos(i))\n",
    "        stem_it.append(lem)\n",
    "        \n",
    "    return stem_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['processed'] = np.nan\n",
    "dat['processed'] = dat['review'].apply(preprocess)\n",
    "dat['processed'] = dat['processed'].apply(lemmatize_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[part, magic, grow, boy, buy, give, new, hornb...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[amaze, detail, every, credit, photographer, b...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[purchase, behalf, dad, always, ask, look, gau...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[everything, really, need, see, offer, hornby,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[collect, glossy, picture, great, nice, still,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[great, book, extremely, useful, insight, futu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[useful, info, someonelike, start, back, hobby...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[well, produce, good, quality, cataloguesuper,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[happy, communication, funkybuys]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[great, buy]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           processed  rating\n",
       "0  [part, magic, grow, boy, buy, give, new, hornb...       4\n",
       "1  [amaze, detail, every, credit, photographer, b...       5\n",
       "2  [purchase, behalf, dad, always, ask, look, gau...       5\n",
       "3  [everything, really, need, see, offer, hornby,...       5\n",
       "4  [collect, glossy, picture, great, nice, still,...       5\n",
       "5  [great, book, extremely, useful, insight, futu...       5\n",
       "6  [useful, info, someonelike, start, back, hobby...       5\n",
       "7  [well, produce, good, quality, cataloguesuper,...       5\n",
       "8                  [happy, communication, funkybuys]       4\n",
       "9                                       [great, buy]       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenating the processed review with ratings column\n",
    "result = pd.concat([dat['processed'],dat['rating']], axis=1)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorizer:  (28212, 28732)\n"
     ]
    }
   ],
   "source": [
    "# Calculating the tf-idf values\n",
    "# Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "X_train_1 = result['processed'].values\n",
    "Y = result['rating'].values\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Just a dummy function.\n",
    "    \"\"\"\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, analyzer='word',preprocessor = identity_tokenizer,lowercase=True)    \n",
    "X = vectorizer.fit_transform(X_train_1)\n",
    "\n",
    "print(\"Shape of the vectorizer: \",X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train model\n",
    "LinearSVC retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "ADASYN retrieved from https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html\n",
    "make_pipeline retrieved from https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.pipeline.make_pipeline.html#imblearn.pipeline.make_pipeline\n",
    "\"\"\"\n",
    "from imblearn.over_sampling import ADASYN                   # to do oversampling\n",
    "from sklearn.svm import LinearSVC                           # to train support vector machine models\n",
    "from imblearn.pipeline import make_pipeline                 # construct a pipeline from given estimators. Automates a machine learning workflow\n",
    "\n",
    "begin = datetime.datetime.now()\n",
    "\n",
    "pipeline = make_pipeline(ADASYN(),LinearSVC())\n",
    "\n",
    "pipeline.fit(X, Y)\n",
    "\n",
    "print(\"Duration for training: \",datetime.datetime.now() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tf-idf model to disk for future use\n",
    "# Retrieved from https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "import pickle\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))\n",
    "pickle.dump(vectorizer.vocabulary_,open(\"feature.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
