{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "import re   # regex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv('review_ver2.csv')\n",
    "dat['processed'] = np.nan\n",
    "dat = dat.drop(columns = ['Unnamed: 0'])\n",
    "print(len(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')    # Expect string\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(review):\n",
    "    result = review.str.replace(r'\\d+', '')    # Remove numbers/ digits\n",
    "    result = result.str.replace(r'\\W', ' ')     # Remove puntuations\n",
    "    val = result.str.lower()     # Convert all the reviews to lowercase \n",
    "    \n",
    "    return val.apply(lambda row: [word for word in row.split() if word not in stop_words])   # tokenize and stop words removal\n",
    "    \n",
    "    \n",
    "def lemmatize_it(series_list):\n",
    "    stem_it = []\n",
    "    for i in series_list:\n",
    "        lem = lemmatizer.lemmatize(i, get_wordnet_pos(i))\n",
    "        stem_it.append(lem)\n",
    "        \n",
    "    return stem_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'magic', 'grow', 'boy', 'buy', 'give', 'new', 'hornby', 'catalogue', 'every', 'year', 'even', 'include', 'product', 'previous', 'year', 'still', 'get', 'old', 'one', 'date', 'back', 'somewhere', 'day', 'catalogue', 'especially', 'informative', 'tell', 'vintage', 'roll', 'stock', 'useful', 'dedicate', 'railway', 'one', 'particular', 'era', 'train', 'company']\n"
     ]
    }
   ],
   "source": [
    "dat['processed'] = preprocess(dat['review'])     # Vectorize method\n",
    "dat['processed'] = dat['processed'].apply(lemmatize_it)\n",
    "print(dat['processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e0ca50c28c2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#changed = nltk.pos_tag(done)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mlem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mstem_it\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \"\"\"\n",
      "\u001b[1;32m<ipython-input-4-c7baaec4e3b4>\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     tag_dict = {\"J\": wordnet.ADJ,\n\u001b[0;32m      6\u001b[0m                 \u001b[1;34m\"N\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             AP_MODEL_LOC = 'file:' + str(\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taggers/averaged_perceptron_tagger/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             )\n\u001b[0;32m    146\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# Is the path item a directory or is resource_name an absolute path?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                 \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl2pathname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Tokenization, convert case, stop words removal and stemming\n",
    "tokenizer = RegexpTokenizer(r'\\w+')    # Expect string\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "for x in range(len(dat['review'])):\n",
    "    result = re.sub(r'\\d+', '', dat['review'][x])\n",
    "    val = result.lower()\n",
    "    t = tokenizer.tokenize(val)\n",
    "    done = [i for i in t if not i in stop_words]\n",
    "    stem_it = []\n",
    "    #changed = nltk.pos_tag(done)\n",
    "    for p in done:\n",
    "        lem = lemmatizer.lemmatize(p, get_wordnet_pos(p))\n",
    "        stem_it.append(lem)\n",
    "        \n",
    "    #for p in done:\n",
    "    #    lem = stemmer.stem(p)\n",
    "    #    stem_it.append(lem)\n",
    "    \n",
    "    dat['processed'][x] = stem_it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>review</th>\n",
       "      <th>product</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>10/10</td>\n",
       "      <td>5</td>\n",
       "      <td>20 Oct. 2014</td>\n",
       "      <td>By\\n\\n    \\n\\n    Alex\\n\\n  \\n\\n on 20 Oct. 2...</td>\n",
       "      <td>I bought two sets of pearl blue dice and to m...</td>\n",
       "      <td>Pearl Grey Set (dice0120)</td>\n",
       "      <td>[bought, two, set, pearl, blue, dice, delight,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Not bad, not great.</td>\n",
       "      <td>3</td>\n",
       "      <td>10 Jan. 2016</td>\n",
       "      <td>By\\n\\n    \\n\\n    cosmogirl\\n\\n  \\n\\n on 10 J...</td>\n",
       "      <td>These are ok dice. Bit small, few irregularit...</td>\n",
       "      <td>Pearl Grey Set (dice0120)</td>\n",
       "      <td>[ok, dice, bit, small, irregularity, number, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>1 Nov. 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    Cherry\\n\\n  \\n\\n on 1 Nov. ...</td>\n",
       "      <td>Super fast delivery and lovely dice! The pris...</td>\n",
       "      <td>Pearl Grey Set (dice0120)</td>\n",
       "      <td>[super, fast, delivery, lovely, dice, prismati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Five Stars</td>\n",
       "      <td>5</td>\n",
       "      <td>22 May 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    Jack Wild\\n\\n  \\n\\n on 22 M...</td>\n",
       "      <td>Good quality dice - bit hard to know what mor...</td>\n",
       "      <td>Pearl Grey Set (dice0120)</td>\n",
       "      <td>[good, quality, dice, bit, hard, know, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>I really like this game it is so simplistic bu...</td>\n",
       "      <td>5</td>\n",
       "      <td>3 July 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    L Moore\\n\\n  \\n\\n on 3 July...</td>\n",
       "      <td>I really like this game it is so simplistic b...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[really, like, game, simplistic, allows, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Tatt...</td>\n",
       "      <td>1</td>\n",
       "      <td>30 Sept. 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    Naomi Ward\\n\\n  \\n\\n on 30 ...</td>\n",
       "      <td>Not what was expected, I know they are only d...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[expect, know, dice, however, look, feel, chea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Harsh and cruel- like The Dinner- but compuls...</td>\n",
       "      <td>3</td>\n",
       "      <td>29 Sept. 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    Patti Crone\\n\\n  \\n\\n on 29...</td>\n",
       "      <td>Harsh and cruel- like The Dinner- but compuls...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[harsh, cruel, like, dinner, compulsive, readi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Good simple game, poor quality product</td>\n",
       "      <td>3</td>\n",
       "      <td>3 Sept. 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    Laura Cook\\n\\n  \\n\\n on 3 S...</td>\n",
       "      <td>Great fun game to play as a group! However th...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[great, fun, game, play, group, however, quali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>games</td>\n",
       "      <td>4</td>\n",
       "      <td>21 Dec. 2014</td>\n",
       "      <td>By\\n\\n    \\n\\n    ancient Pete\\n\\n  \\n\\n on 2...</td>\n",
       "      <td>Not the swiftist delivery but bear in mind it...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[swiftist, delivery, bear, mind, come, direct,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>... can't rate the product 5 stars as I was d...</td>\n",
       "      <td>4</td>\n",
       "      <td>9 Jun. 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    AlanM\\n\\n  \\n\\n on 9 Jun. 2...</td>\n",
       "      <td>Unfortunately I can't rate the product 5 star...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[unfortunately, rate, product, star, disappoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Poor craftmanship, the dices were different s...</td>\n",
       "      <td>1</td>\n",
       "      <td>13 Feb. 2016</td>\n",
       "      <td>By\\n\\n    \\n\\n    Iain\\n\\n  \\n\\n on 13 Feb. 2...</td>\n",
       "      <td>Poor craftmanship, the dices were different s...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[poor, craftmanship, dice, different, size, et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Well done</td>\n",
       "      <td>5</td>\n",
       "      <td>14 Dec. 2015</td>\n",
       "      <td>By\\n\\n    \\n\\n    Frederick L.\\n\\n  \\n\\n on 1...</td>\n",
       "      <td>Item better than expected, dice indented with...</td>\n",
       "      <td>SmartDealsPro LCR Dice Game Left Right Center ...</td>\n",
       "      <td>[item, well, expect, dice, indent, symbol, pai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title rating  \\\n",
       "988                                             10/10       5   \n",
       "989                               Not bad, not great.       3   \n",
       "990                                        Five Stars       5   \n",
       "991                                        Five Stars       5   \n",
       "992  I really like this game it is so simplistic bu...      5   \n",
       "993                                           Tatt...       1   \n",
       "994   Harsh and cruel- like The Dinner- but compuls...      3   \n",
       "995            Good simple game, poor quality product       3   \n",
       "996                                             games       4   \n",
       "997   ... can't rate the product 5 stars as I was d...      4   \n",
       "998   Poor craftmanship, the dices were different s...      1   \n",
       "999                                         Well done       5   \n",
       "\n",
       "                date                                             author  \\\n",
       "988    20 Oct. 2014    By\\n\\n    \\n\\n    Alex\\n\\n  \\n\\n on 20 Oct. 2...   \n",
       "989    10 Jan. 2016    By\\n\\n    \\n\\n    cosmogirl\\n\\n  \\n\\n on 10 J...   \n",
       "990     1 Nov. 2015    By\\n\\n    \\n\\n    Cherry\\n\\n  \\n\\n on 1 Nov. ...   \n",
       "991     22 May 2015    By\\n\\n    \\n\\n    Jack Wild\\n\\n  \\n\\n on 22 M...   \n",
       "992     3 July 2015    By\\n\\n    \\n\\n    L Moore\\n\\n  \\n\\n on 3 July...   \n",
       "993   30 Sept. 2015    By\\n\\n    \\n\\n    Naomi Ward\\n\\n  \\n\\n on 30 ...   \n",
       "994   29 Sept. 2015    By\\n\\n    \\n\\n    Patti Crone\\n\\n  \\n\\n on 29...   \n",
       "995    3 Sept. 2015    By\\n\\n    \\n\\n    Laura Cook\\n\\n  \\n\\n on 3 S...   \n",
       "996    21 Dec. 2014    By\\n\\n    \\n\\n    ancient Pete\\n\\n  \\n\\n on 2...   \n",
       "997     9 Jun. 2015    By\\n\\n    \\n\\n    AlanM\\n\\n  \\n\\n on 9 Jun. 2...   \n",
       "998    13 Feb. 2016    By\\n\\n    \\n\\n    Iain\\n\\n  \\n\\n on 13 Feb. 2...   \n",
       "999    14 Dec. 2015    By\\n\\n    \\n\\n    Frederick L.\\n\\n  \\n\\n on 1...   \n",
       "\n",
       "                                                review  \\\n",
       "988   I bought two sets of pearl blue dice and to m...   \n",
       "989   These are ok dice. Bit small, few irregularit...   \n",
       "990   Super fast delivery and lovely dice! The pris...   \n",
       "991   Good quality dice - bit hard to know what mor...   \n",
       "992   I really like this game it is so simplistic b...   \n",
       "993   Not what was expected, I know they are only d...   \n",
       "994   Harsh and cruel- like The Dinner- but compuls...   \n",
       "995   Great fun game to play as a group! However th...   \n",
       "996   Not the swiftist delivery but bear in mind it...   \n",
       "997   Unfortunately I can't rate the product 5 star...   \n",
       "998   Poor craftmanship, the dices were different s...   \n",
       "999   Item better than expected, dice indented with...   \n",
       "\n",
       "                                               product  \\\n",
       "988                          Pearl Grey Set (dice0120)   \n",
       "989                          Pearl Grey Set (dice0120)   \n",
       "990                          Pearl Grey Set (dice0120)   \n",
       "991                          Pearl Grey Set (dice0120)   \n",
       "992  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "993  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "994  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "995  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "996  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "997  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "998  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "999  SmartDealsPro LCR Dice Game Left Right Center ...   \n",
       "\n",
       "                                             processed  \n",
       "988  [bought, two, set, pearl, blue, dice, delight,...  \n",
       "989  [ok, dice, bit, small, irregularity, number, p...  \n",
       "990  [super, fast, delivery, lovely, dice, prismati...  \n",
       "991        [good, quality, dice, bit, hard, know, say]  \n",
       "992  [really, like, game, simplistic, allows, every...  \n",
       "993  [expect, know, dice, however, look, feel, chea...  \n",
       "994  [harsh, cruel, like, dinner, compulsive, readi...  \n",
       "995  [great, fun, game, play, group, however, quali...  \n",
       "996  [swiftist, delivery, bear, mind, come, direct,...  \n",
       "997  [unfortunately, rate, product, star, disappoin...  \n",
       "998  [poor, craftmanship, dice, different, size, et...  \n",
       "999  [item, well, expect, dice, indent, symbol, pai...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.iloc[988: 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf using built-in function\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def dummy_func(docs):\n",
    "    return docs\n",
    "\n",
    "X_1 = dat['processed'].values\n",
    "Y_train = dat['rating'].values\n",
    "print(len(X_1), len(Y_train))\n",
    "\n",
    "#concatenating the new data frame\n",
    "vectorizer = TfidfVectorizer(analyzer='word',tokenizer=dummy_func, preprocessor=dummy_func, token_pattern=None)\n",
    "X = vectorizer.fit_transform(X_1)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "corpus_index = [n for n in range(len(X_1))]\n",
    "rows, cols = X.nonzero()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords:\n",
      "yes 0.692\n",
      "worth 0.672\n",
      "work 0.686\n",
      "wonderful 0.693\n",
      "want 0.673\n",
      "verygood 1.0\n",
      "useful 0.884\n",
      "use 0.734\n",
      "tricky 1.0\n",
      "traitor 1.0\n",
      "tnx 1.0\n",
      "tiny 1.0\n",
      "tin 0.674\n",
      "thankyou 0.671\n",
      "thanku 1.0\n",
      "thanks 0.669\n",
      "thank 0.691\n",
      "sweet 0.702\n",
      "suuuuperb 1.0\n",
      "suppose 0.727\n",
      "superseller 1.0\n",
      "superb 0.675\n",
      "super 0.667\n",
      "sturdy 0.687\n",
      "state 0.666\n",
      "spot 0.701\n",
      "splendid 0.835\n",
      "small 0.677\n",
      "sick 1.0\n",
      "show 0.696\n",
      "say 0.698\n",
      "satisfied 0.678\n",
      "satisfaction 0.718\n",
      "sand 0.669\n",
      "rockin 1.0\n",
      "robust 0.692\n",
      "requiredto 1.0\n",
      "require 0.758\n",
      "recommend 0.679\n",
      "quick 0.715\n",
      "quality 0.67\n",
      "product 0.678\n",
      "problem 0.667\n",
      "pretty 0.711\n",
      "present 0.669\n",
      "pleased 0.669\n",
      "picture 0.678\n",
      "phantastic 1.0\n",
      "perfect 0.666\n",
      "perfeck 1.0\n",
      "pb 0.743\n",
      "outstanding 0.722\n",
      "okay 0.697\n",
      "ok 0.706\n",
      "noice 0.758\n",
      "nice 0.667\n",
      "need 0.7\n",
      "meant 1.0\n",
      "lush 0.793\n",
      "lovey 0.909\n",
      "lovely 0.689\n",
      "love 0.73\n",
      "look 0.669\n",
      "lol 1.0\n",
      "like 0.668\n",
      "leak 0.895\n",
      "k 0.68\n",
      "job 0.682\n",
      "issue 0.923\n",
      "impressive 0.839\n",
      "impressed 0.667\n",
      "impeccable 1.0\n",
      "ideal 0.671\n",
      "hoooooooooo 1.0\n",
      "hey 0.667\n",
      "happy 0.667\n",
      "gud 1.0\n",
      "grrat 1.0\n",
      "greay 1.0\n",
      "great 0.668\n",
      "gr 0.908\n",
      "gorgeous 0.738\n",
      "good 0.714\n",
      "glow 0.704\n",
      "glad 1.0\n",
      "gift 0.703\n",
      "ggod 1.0\n",
      "g 1.0\n",
      "funny 1.0\n",
      "fun 0.671\n",
      "fitsperfectly 1.0\n",
      "fine 0.689\n",
      "fast 0.708\n",
      "fantastic 0.683\n",
      "fabulous 0.674\n",
      "fab 0.67\n",
      "expensive 0.671\n",
      "expellant 1.0\n",
      "expect 0.668\n",
      "exellent 1.0\n",
      "exelent 0.875\n",
      "excenalt 1.0\n",
      "excellent 0.67\n",
      "excellant 0.759\n",
      "excelent 0.789\n",
      "exactly 0.684\n",
      "epik 1.0\n",
      "enjoyable 1.0\n",
      "enjoy 0.746\n",
      "empresive 1.0\n",
      "e 1.0\n",
      "dreat 1.0\n",
      "discribed 0.888\n",
      "disappointed 0.698\n",
      "description 0.789\n",
      "described 0.672\n",
      "delighted 0.678\n",
      "cute 0.688\n",
      "cuddly 0.698\n",
      "crap 1.0\n",
      "cool 0.666\n",
      "complete 0.711\n",
      "complaint 0.691\n",
      "coddley 1.0\n",
      "classic 0.856\n",
      "chuffed 0.681\n",
      "cheap 0.686\n",
      "brilliant 0.666\n",
      "brillant 0.784\n",
      "brill 0.668\n",
      "briliant 1.0\n",
      "bril 1.0\n",
      "best 0.802\n",
      "beautiful 0.699\n",
      "batman 1.0\n",
      "bargain 0.667\n",
      "bakugan 0.712\n",
      "bad 0.666\n",
      "baby 0.674\n",
      "awsome 0.825\n",
      "awesome 0.672\n",
      "ask 1.0\n",
      "asdescribedperfect 1.0\n",
      "aok 1.0\n",
      "amaze 0.717\n",
      "advertised 0.669\n",
      "ace 0.884\n",
      "aaaa 1.0\n",
      "aaa 0.706\n",
      "ditto 0.991\n",
      "b 0.981\n",
      "sirvice 0.975\n",
      "dashingly 0.975\n",
      "servivea 0.974\n",
      "ebayer 0.974\n",
      "aquasition 0.974\n",
      "grandaugther 0.974\n",
      "intresting 0.973\n",
      "wright 0.972\n",
      "likness 0.972\n",
      "tomica 0.97\n",
      "chessboard 0.968\n",
      "transmitter 0.966\n",
      "yay 0.965\n",
      "em 0.672\n",
      "retro 0.96\n",
      "neice 0.805\n",
      "grandaughters 0.96\n",
      "yeah 0.77\n",
      "pressy 0.958\n",
      "dvd 0.957\n",
      "garage 0.957\n",
      "lovly 0.824\n",
      "hubby 0.688\n",
      "godson 0.749\n",
      "sun 0.955\n",
      "blah 0.954\n",
      "ty 0.7\n",
      "likeness 0.756\n",
      "lorry 0.68\n",
      "girlfriend 0.951\n",
      "grt 0.95\n",
      "joke 0.675\n",
      "presentation 0.95\n",
      "x 0.723\n",
      "lad 0.687\n",
      "disk 0.946\n",
      "wonderfull 0.944\n",
      "oooh 0.944\n",
      "lovd 0.944\n",
      "v 0.731\n",
      "rounď 0.943\n",
      "partner 0.734\n",
      "boat 0.738\n",
      "halloween 0.739\n",
      "beatyfull 0.94\n",
      "wife 0.74\n",
      "graet 0.937\n",
      "error 0.936\n",
      "grandaughter 0.676\n",
      "descibed 0.707\n",
      "truck 0.69\n",
      "replacement 0.935\n",
      "mould 0.935\n",
      "craft 0.69\n",
      "jubly 0.934\n",
      "qulaity 0.934\n",
      "thqnk 0.934\n",
      "presentthank 0.934\n",
      "stash 0.932\n",
      "kite 0.67\n",
      "offer 0.931\n",
      "stuff 0.763\n",
      "mum 0.693\n",
      "nendoroid 0.929\n",
      "housemate 0.929\n",
      "decopatch 0.929\n",
      "colletion 0.929\n",
      "bead 0.673\n",
      "challenge 0.712\n",
      "reccomend 0.925\n",
      "class 0.698\n",
      "schedule 0.812\n",
      "costume 0.668\n",
      "thanx 0.73\n",
      "husband 0.666\n",
      "luv 0.922\n",
      "banner 0.669\n",
      "reusable 0.922\n",
      "grandchild 0.692\n",
      "damage 0.674\n",
      "material 0.92\n",
      "mask 0.678\n",
      "dnd 0.92\n",
      "fundraise 0.92\n",
      "bf 0.92\n",
      "deck 0.919\n",
      "niece 0.688\n",
      "ut 0.918\n",
      "pool 0.743\n",
      "condition 0.671\n",
      "fidgety 0.914\n",
      "shrek 0.913\n",
      "hilarious 0.913\n",
      "moneybank 0.912\n",
      "balloon 0.668\n",
      "doll 0.672\n",
      "delightful 0.696\n",
      "do 0.672\n",
      "brithday 0.91\n",
      "eyepins 0.91\n",
      "hardy 0.695\n",
      "puzzle 0.672\n",
      "tease 0.909\n",
      "helpful 0.908\n",
      "prezzy 0.908\n",
      "pressie 0.721\n",
      "badge 0.734\n",
      "photo 0.675\n",
      "weise 0.906\n",
      "littleboy 0.906\n",
      "kit 0.669\n",
      "effective 0.703\n",
      "hard 0.905\n",
      "nephew 0.684\n",
      "xxx 0.706\n",
      "granddaughter 0.674\n",
      "daughty 0.903\n",
      "addition 0.695\n",
      "brillante 0.901\n",
      "coulour 0.901\n",
      "swirl 0.9\n",
      "hacker 0.9\n",
      "frequently 0.9\n",
      "exquisite 0.9\n",
      "safely 0.669\n",
      "always 0.682\n",
      "dice 0.674\n",
      "oollection 0.897\n",
      "vert 0.897\n",
      "puppet 0.669\n",
      "duurable 0.896\n",
      "preschool 0.754\n",
      "reorder 0.89\n",
      "officent 0.895\n",
      "responsibly 0.894\n",
      "suposed 0.894\n",
      "promise 0.894\n",
      "family 0.68\n",
      "strongly 0.768\n",
      "service 0.676\n",
      "okie 0.893\n",
      "footie 0.892\n",
      "munsters 0.89\n",
      "overprice 0.685\n",
      "ex 0.89\n",
      "design 0.889\n",
      "grandad 0.669\n",
      "absolutely 0.688\n",
      "car 0.666\n",
      "verry 0.872\n",
      "nicely 0.846\n",
      "enthusiast 0.886\n",
      "poor 0.885\n",
      "pack 0.885\n",
      "fishnice 0.885\n",
      "ver 0.884\n",
      "beautifull 0.884\n",
      "butt 0.685\n",
      "paint 0.883\n",
      "loop 0.882\n",
      "moon 0.687\n",
      "dexterity 0.881\n",
      "charger 0.683\n",
      "fan 0.701\n",
      "minature 0.682\n",
      "stayed 0.879\n",
      "bloomin 0.879\n",
      "party 0.699\n",
      "detail 0.682\n",
      "vaue 0.719\n",
      "indeed 0.694\n",
      "monthhold 0.878\n",
      "thumb 0.878\n",
      "boy 0.667\n",
      "puppy 0.877\n",
      "reward 0.876\n",
      "recomended 0.876\n",
      "fork 0.876\n",
      "scalextrix 0.876\n",
      "miniature 0.678\n",
      "sooooo 0.75\n",
      "friend 0.874\n",
      "cavilier 0.874\n",
      "loud 0.874\n",
      "wat 0.874\n",
      "spoon 0.873\n",
      "bunting 0.742\n",
      "qualit 0.873\n",
      "colour 0.666\n",
      "duplicate 0.866\n",
      "casino 0.848\n",
      "collectible 0.794\n",
      "fit 0.681\n",
      "model 0.691\n",
      "thrill 0.707\n",
      "gandson 0.87\n",
      "storio 0.87\n",
      "magnetic 0.869\n",
      "figure 0.691\n",
      "cat 0.672\n",
      "wrap 0.867\n",
      "battling 0.866\n",
      "joyed 0.866\n",
      "provider 0.865\n",
      "neat 0.865\n",
      "broked 0.865\n",
      "card 0.668\n",
      "deal 0.705\n",
      "purchase 0.703\n",
      "kato 0.863\n",
      "hop 0.703\n",
      "pthis 0.862\n",
      "educational 0.692\n",
      "wither 0.862\n",
      "kill 0.862\n",
      "gappy 0.861\n",
      "yet 0.685\n",
      "mygrandson 0.861\n",
      "crack 0.758\n",
      "autobot 0.86\n",
      "thay 0.86\n",
      "completes 0.86\n",
      "end 0.86\n",
      "grandson 0.666\n",
      "kid 0.666\n",
      "quite 0.735\n",
      "masher 0.859\n",
      "vvery 0.859\n",
      "mold 0.859\n",
      "sausage 0.859\n",
      "recipient 0.669\n",
      "success 0.764\n",
      "thin 0.858\n",
      "creative 0.718\n",
      "load 0.724\n",
      "bicycle 0.857\n",
      "balance 0.856\n",
      "peoductt 0.856\n",
      "ornament 0.856\n",
      "birthday 0.72\n",
      "speedy 0.669\n",
      "mouse 0.779\n",
      "packed 0.855\n",
      "flimsy 0.686\n",
      "slick 0.854\n",
      "live 0.854\n",
      "webkinz 0.853\n",
      "portable 0.81\n",
      "glass 0.733\n",
      "pencil 0.673\n",
      "poundland 0.852\n",
      "please 0.829\n",
      "efficient 0.726\n",
      "superfast 0.851\n",
      "top 0.674\n",
      "skylanders 0.851\n",
      "money 0.67\n",
      "sensitive 0.851\n",
      "layput 0.851\n",
      "helicopter 0.85\n",
      "entertain 0.85\n",
      "inflate 0.788\n",
      "stationary 0.85\n",
      "oap 0.849\n",
      "boyfriend 0.68\n",
      "activity 0.848\n",
      "propeller 0.848\n",
      "tesco 0.848\n",
      "value 0.701\n",
      "plant 0.848\n",
      "yearold 0.848\n",
      "interactive 0.847\n",
      "item 0.674\n",
      "puzzler 0.847\n",
      "bowl 0.847\n",
      "template 0.846\n",
      "goog 0.846\n",
      "treat 0.68\n",
      "accord 0.845\n",
      "game 0.668\n",
      "outfit 0.774\n",
      "ha 0.676\n",
      "stardust 0.844\n",
      "smoke 0.844\n",
      "flimsier 0.843\n",
      "expectedprompt 0.843\n",
      "bear 0.673\n",
      "wiyh 0.842\n",
      "arive 0.842\n",
      "buy 0.681\n",
      "adores 0.841\n",
      "suit 0.841\n",
      "grate 0.84\n",
      "nothing 0.84\n",
      "leonardo 0.84\n",
      "purpose 0.729\n",
      "economic 0.839\n",
      "scary 0.765\n",
      "blew 0.839\n",
      "crutch 0.839\n",
      "joy 0.723\n",
      "collection 0.697\n",
      "chuiffed 0.838\n",
      "cheaper 0.838\n",
      "fillet 0.838\n",
      "pricey 0.674\n",
      "development 0.838\n",
      "rainy 0.758\n",
      "handbag 0.838\n",
      "shipment 0.837\n",
      "video 0.837\n",
      "relax 0.836\n",
      "qwik 0.836\n",
      "girl 0.695\n",
      "presants 0.835\n",
      "christen 0.835\n",
      "clics 0.835\n",
      "cheerful 0.721\n",
      "blind 0.834\n",
      "diecast 0.834\n",
      "fault 0.681\n",
      "quiz 0.675\n",
      "beed 0.833\n",
      "ot 0.833\n",
      "easy 0.681\n",
      "pen 0.667\n",
      "sister 0.696\n",
      "plastic 0.831\n",
      "daughter 0.667\n",
      "dane 0.831\n",
      "bale 0.83\n",
      "sew 0.83\n",
      "pattern 0.731\n",
      "bullyland 0.829\n",
      "misshapen 0.829\n",
      "calendar 0.829\n",
      "anytime 0.829\n",
      "merchandise 0.829\n",
      "set 0.679\n",
      "statue 0.689\n",
      "excite 0.828\n",
      "price 0.671\n",
      "birhtday 0.828\n",
      "hotwheels 0.69\n",
      "scalextrics 0.828\n",
      "defo 0.706\n",
      "baloons 0.827\n",
      "swiftly 0.759\n",
      "smencils 0.827\n",
      "daily 0.827\n",
      "thought 0.688\n",
      "wagon 0.826\n",
      "beyblade 0.676\n",
      "part 0.779\n",
      "transaction 0.826\n",
      "extremely 0.669\n",
      "daugter 0.826\n",
      "song 0.826\n",
      "crumple 0.825\n",
      "child 0.683\n",
      "animator 0.825\n",
      "toy 0.686\n",
      "grandkids 0.714\n",
      "jigsaw 0.676\n",
      "adorable 0.758\n",
      "draw 0.823\n",
      "masquarade 0.823\n",
      "paper 0.823\n",
      "dress 0.823\n",
      "wrong 0.68\n",
      "r 0.699\n",
      "ej 0.823\n",
      "delievered 0.822\n",
      "punch 0.822\n",
      "le 0.821\n",
      "bouqet 0.821\n",
      "ried 0.821\n",
      "guy 0.821\n",
      "shorter 0.821\n",
      "saw 0.821\n",
      "recipiant 0.821\n",
      "spacer 0.821\n",
      "brilliantly 0.821\n",
      "package 0.668\n",
      "cabinet 0.82\n",
      "mic 0.82\n",
      "smurf 0.82\n",
      "satisfactory 0.778\n",
      "crufix 0.82\n",
      "pro 0.819\n",
      "gratefully 0.819\n",
      "craftwork 0.819\n",
      "silly 0.818\n",
      "aqua 0.818\n",
      "sticker 0.666\n",
      "device 0.818\n",
      "toaster 0.818\n",
      "twilight 0.817\n",
      "wantedcool 0.817\n",
      "bigwig 0.817\n",
      "gw 0.817\n",
      "garland 0.817\n",
      "dude 0.817\n",
      "informs 0.817\n",
      "toggle 0.816\n",
      "planner 0.816\n",
      "greatmodel 0.815\n",
      "al 0.815\n",
      "goood 0.815\n",
      "xx 0.714\n",
      "clean 0.814\n",
      "express 0.814\n",
      "belive 0.814\n",
      "lifetime 0.814\n",
      "laugh 0.679\n",
      "telehandler 0.813\n",
      "lifelike 0.749\n",
      "frog 0.813\n",
      "granson 0.709\n",
      "chip 0.812\n",
      "selection 0.811\n",
      "image 0.718\n",
      "loki 0.812\n",
      "maintenance 0.812\n",
      "perfert 0.803\n",
      "beauty 0.812\n",
      "shipping 0.68\n",
      "scalextric 0.667\n",
      "servo 0.811\n",
      "depth 0.811\n",
      "frustrate 0.811\n",
      "mini 0.76\n",
      "ekki 0.81\n",
      "supper 0.81\n",
      "son 0.699\n",
      "hit 0.809\n",
      "wet 0.809\n",
      "changer 0.809\n",
      "brightly 0.693\n",
      "everyone 0.74\n",
      "dh 0.809\n",
      "abit 0.707\n",
      "broke 0.809\n",
      "international 0.808\n",
      "vase 0.668\n",
      "lannisters 0.808\n",
      "architectural 0.808\n",
      "outdoors 0.736\n",
      "weight 0.702\n",
      "rather 0.807\n",
      "lcg 0.807\n",
      "quilling 0.807\n",
      "surprise 0.776\n",
      "authentic 0.788\n",
      "requirement 0.694\n",
      "diego 0.806\n",
      "invite 0.668\n",
      "cm 0.713\n",
      "qulity 0.805\n",
      "build 0.675\n",
      "accurate 0.805\n",
      "bagpuss 0.805\n",
      "prince 0.805\n",
      "cracker 0.743\n",
      "fayre 0.804\n",
      "dog 0.734\n",
      "everything 0.719\n",
      "grand 0.666\n",
      "teddy 0.711\n",
      "muppets 0.804\n",
      "pug 0.721\n",
      "sob 0.804\n",
      "bright 0.669\n",
      "unopened 0.803\n",
      "realistic 0.666\n",
      "nonetheless 0.803\n",
      "xmas 0.683\n",
      "highly 0.803\n",
      "graphic 0.671\n",
      "wish 0.671\n",
      "band 0.677\n",
      "mth 0.801\n",
      "teacher 0.801\n",
      "celebration 0.801\n",
      "sonvery 0.801\n",
      "lay 0.801\n",
      "today 0.686\n",
      "artwork 0.801\n",
      "puchase 0.801\n",
      "play 0.8\n",
      "chain 0.8\n",
      "decribed 0.8\n",
      "mike 0.68\n",
      "really 0.677\n",
      "rebel 0.8\n",
      "successful 0.8\n",
      "riot 0.8\n",
      "tremendous 0.8\n",
      "trans 0.8\n",
      "adore 0.8\n",
      "de 0.799\n",
      "preserve 0.799\n",
      "figurine 0.779\n",
      "mess 0.725\n",
      "bike 0.799\n",
      "counter 0.798\n",
      "shuffle 0.798\n",
      "adition 0.798\n",
      "rummikub 0.798\n",
      "priduct 0.798\n",
      "courteously 0.798\n",
      "stepson 0.797\n",
      "beyoncé 0.797\n",
      "xenomorph 0.797\n",
      "spread 0.796\n",
      "mam 0.796\n",
      "pippi 0.796\n",
      "toddler 0.696\n",
      "amount 0.795\n",
      "tacky 0.795\n",
      "sherlock 0.795\n",
      "lookforward 0.795\n",
      "autusm 0.794\n",
      "stocking 0.794\n",
      "mario 0.783\n",
      "gizmo 0.794\n",
      "reall 0.794\n",
      "wll 0.793\n",
      "right 0.706\n",
      "man 0.793\n",
      "original 0.793\n",
      "post 0.793\n",
      "amazingly 0.727\n",
      "intime 0.792\n",
      "gig 0.792\n",
      "prix 0.792\n",
      "vision 0.791\n",
      "yr 0.791\n",
      "virgin 0.791\n",
      "piston 0.791\n",
      "print 0.72\n",
      "player 0.738\n",
      "story 0.79\n",
      "tyvm 0.79\n",
      "opulent 0.79\n",
      "clunky 0.79\n",
      "ahead 0.789\n",
      "brought 0.789\n",
      "luvly 0.789\n",
      "bey 0.694\n",
      "vfm 0.757\n",
      "ive 0.789\n",
      "dolly 0.773\n",
      "jake 0.788\n",
      "naught 0.788\n",
      "third 0.788\n",
      "five 0.788\n",
      "user 0.788\n",
      "lego 0.787\n",
      "hour 0.681\n",
      "cube 0.673\n",
      "championship 0.787\n",
      "seaking 0.787\n",
      "charm 0.757\n",
      "sooo 0.787\n",
      "still 0.787\n",
      "surprisingly 0.786\n",
      "rugged 0.786\n",
      "sorry 0.786\n",
      "truly 0.786\n",
      "bindis 0.786\n",
      "transformer 0.786\n",
      "arlo 0.786\n",
      "rapidly 0.785\n",
      "grippy 0.785\n",
      "productgreat 0.785\n",
      "have 0.785\n",
      "year 0.673\n",
      "atc 0.785\n",
      "lightsabers 0.785\n",
      "movie 0.669\n",
      "dark 0.784\n",
      "schlich 0.784\n",
      "private 0.784\n",
      "receipt 0.783\n",
      "soft 0.721\n",
      "chunky 0.783\n",
      "ppackaging 0.783\n",
      "geat 0.783\n",
      "dose 0.783\n",
      "glove 0.782\n",
      "week 0.782\n",
      "deliveredgood 0.782\n",
      "minty 0.782\n",
      "nut 0.782\n",
      "jig 0.782\n",
      "walker 0.782\n",
      "festival 0.781\n",
      "one 0.781\n",
      "intriquet 0.781\n",
      "stil 0.781\n",
      "charizard 0.78\n",
      "perfectly 0.678\n",
      "penguin 0.676\n",
      "incredible 0.78\n",
      "woukd 0.749\n",
      "fly 0.779\n",
      "batbie 0.779\n",
      "dragon 0.668\n",
      "bold 0.778\n",
      "nearly 0.775\n",
      "pitty 0.778\n",
      "dugh 0.778\n",
      "addict 0.777\n",
      "jhs 0.777\n",
      "discription 0.777\n",
      "cutlery 0.777\n",
      "buyer 0.776\n",
      "comfortable 0.705\n",
      "none 0.776\n",
      "appreciate 0.776\n",
      "sensibly 0.775\n",
      "fyi 0.775\n",
      "durability 0.774\n",
      "minion 0.671\n",
      "barlow 0.774\n",
      "arrival 0.774\n",
      "producked 0.774\n",
      "bracelet 0.774\n",
      "cousin 0.721\n",
      "coin 0.774\n",
      "daniel 0.71\n",
      "aircraft 0.773\n",
      "rugrats 0.773\n",
      "apply 0.773\n",
      "comppaint 0.772\n",
      "finish 0.772\n",
      "sufficient 0.772\n",
      "complain 0.772\n",
      "spikey 0.772\n",
      "wow 0.772\n",
      "huntington 0.772\n",
      "interior 0.771\n",
      "eat 0.771\n",
      "large 0.771\n",
      "ir 0.771\n",
      "nevertheless 0.77\n",
      "mymums 0.77\n",
      "wolf 0.77\n",
      "pirate 0.697\n",
      "mix 0.769\n",
      "microphone 0.769\n",
      "stop 0.717\n",
      "puzzel 0.768\n",
      "surporter 0.768\n",
      "pokemon 0.671\n",
      "jewellery 0.768\n",
      "trefl 0.768\n",
      "maxi 0.768\n",
      "clever 0.768\n",
      "folder 0.725\n",
      "ubiquitous 0.767\n",
      "scientific 0.767\n",
      "bookmark 0.719\n",
      "howard 0.767\n",
      "ass 0.767\n",
      "helpfull 0.767\n",
      "cart 0.767\n",
      "deliverythank 0.766\n",
      "ogre 0.766\n",
      "extortionate 0.711\n",
      "interest 0.766\n",
      "cant 0.765\n",
      "percy 0.765\n",
      "shoe 0.765\n",
      "shock 0.674\n",
      "oved 0.765\n",
      "pull 0.765\n",
      "digger 0.765\n",
      "hollywood 0.765\n",
      "early 0.666\n",
      "goldfish 0.764\n",
      "protect 0.764\n",
      "roy 0.764\n",
      "detailed 0.724\n",
      "quickley 0.763\n",
      "oxford 0.763\n",
      "generate 0.743\n",
      "heaven 0.763\n",
      "frisbee 0.763\n",
      "could 0.762\n",
      "vehicle 0.672\n",
      "haul 0.762\n",
      "crow 0.762\n",
      "word 0.686\n",
      "serve 0.706\n",
      "often 0.762\n",
      "bus 0.685\n",
      "collectable 0.761\n",
      "chef 0.689\n",
      "worthwhile 0.761\n",
      "aquaplay 0.761\n",
      "proof 0.761\n",
      "reproduction 0.761\n",
      "jessie 0.761\n",
      "mint 0.761\n",
      "glitter 0.703\n",
      "preaty 0.761\n",
      "funkybuys 0.761\n",
      "flim 0.76\n",
      "russell 0.76\n",
      "cover 0.76\n",
      "bouiight 0.76\n",
      "simply 0.76\n",
      "librairy 0.76\n",
      "jack 0.759\n",
      "sylvanian 0.759\n",
      "scare 0.759\n",
      "mumble 0.758\n",
      "buffer 0.758\n",
      "response 0.758\n",
      "dodge 0.701\n",
      "throw 0.758\n",
      "reasonable 0.676\n",
      "decent 0.701\n",
      "science 0.757\n",
      "avenger 0.754\n",
      "durable 0.757\n",
      "haloween 0.756\n",
      "tamamo 0.756\n",
      "unusual 0.756\n",
      "supplier 0.679\n",
      "smal 0.756\n",
      "medium 0.755\n",
      "owl 0.755\n",
      "order 0.674\n",
      "pegasus 0.755\n",
      "staff 0.755\n",
      "school 0.755\n",
      "jazz 0.755\n",
      "rapid 0.673\n",
      "seem 0.667\n",
      "asset 0.667\n",
      "rd 0.754\n",
      "concentration 0.754\n",
      "dispatch 0.699\n",
      "helmet 0.753\n",
      "shark 0.753\n",
      "pickup 0.753\n",
      "visually 0.753\n",
      "master 0.753\n",
      "waterplay 0.753\n",
      "fairy 0.752\n",
      "hex 0.752\n",
      "speedily 0.752\n",
      "costumer 0.752\n",
      "qhite 0.752\n",
      "playmobile 0.674\n",
      "burst 0.705\n",
      "hannah 0.751\n",
      "awfully 0.751\n",
      "promo 0.751\n",
      "safe 0.751\n",
      "gap 0.75\n",
      "zyudenshi 0.75\n",
      "meet 0.69\n",
      "realy 0.75\n",
      "cheep 0.75\n",
      "guitar 0.699\n",
      "solid 0.75\n",
      "bday 0.75\n",
      "regardssebastian 0.75\n",
      "bean 0.749\n",
      "collect 0.749\n",
      "rubbish 0.749\n",
      "met 0.701\n",
      "add 0.748\n",
      "recomend 0.675\n",
      "esay 0.748\n",
      "chess 0.686\n",
      "size 0.696\n",
      "molly 0.747\n",
      "lot 0.747\n",
      "resource 0.684\n",
      "wake 0.747\n",
      "keep 0.747\n",
      "hell 0.747\n",
      "wonted 0.746\n",
      "aon 0.746\n",
      "ambulance 0.746\n",
      "efficiently 0.746\n",
      "stencil 0.746\n",
      "dungeon 0.746\n",
      "personal 0.745\n",
      "doc 0.745\n",
      "sack 0.698\n",
      "diesel 0.745\n",
      "estimate 0.745\n",
      "fluxx 0.745\n",
      "kurboh 0.745\n",
      "uesfull 0.745\n",
      "vegetable 0.745\n",
      "much 0.67\n",
      "flamingo 0.698\n",
      "replica 0.704\n",
      "hand 0.744\n",
      "nano 0.744\n",
      "unicorn 0.744\n",
      "phase 0.744\n",
      "den 0.744\n",
      "exspensive 0.743\n",
      "lola 0.743\n",
      "tatoo 0.743\n",
      "homecoming 0.743\n",
      "thease 0.743\n",
      "bobble 0.742\n",
      "marshal 0.742\n",
      "functional 0.742\n",
      "usual 0.742\n",
      "maisto 0.741\n",
      "last 0.697\n",
      "eurovision 0.705\n",
      "layout 0.667\n",
      "dainty 0.741\n",
      "sufficiently 0.741\n",
      "stoke 0.741\n",
      "choice 0.741\n",
      "booster 0.74\n",
      "george 0.74\n",
      "box 0.68\n",
      "allit 0.74\n",
      "tile 0.689\n",
      "bang 0.74\n",
      "quailty 0.74\n",
      "transfer 0.74\n",
      "alround 0.74\n",
      "pendulum 0.74\n",
      "badass 0.739\n",
      "modders 0.739\n",
      "dosnt 0.739\n",
      "purse 0.739\n",
      "broken 0.739\n",
      "wedding 0.672\n",
      "battery 0.739\n",
      "youtu 0.738\n",
      "cannon 0.738\n",
      "paidi 0.738\n",
      "mm 0.738\n",
      "purchasing 0.738\n",
      "ons 0.738\n",
      "chose 0.701\n",
      "mall 0.738\n",
      "zoo 0.738\n",
      "busy 0.737\n",
      "restore 0.737\n",
      "student 0.737\n",
      "vinyl 0.737\n",
      "dragonborn 0.737\n",
      "young 0.737\n",
      "furby 0.737\n",
      "deliverd 0.737\n",
      "atttractive 0.737\n",
      "quicker 0.737\n",
      "poorly 0.737\n",
      "designstrong 0.737\n",
      "thy 0.736\n",
      "yoohoos 0.736\n",
      "tractor 0.679\n",
      "dimension 0.736\n",
      "grancdchildren 0.736\n",
      "empty 0.736\n",
      "pose 0.736\n",
      "scent 0.702\n",
      "vue 0.736\n",
      "christmas 0.675\n",
      "rody 0.735\n",
      "holographic 0.735\n",
      "cribbage 0.735\n",
      "tour 0.735\n",
      "dissapoint 0.735\n",
      "celebrate 0.735\n",
      "mant 0.734\n",
      "scream 0.734\n",
      "lace 0.734\n",
      "ludo 0.734\n",
      "trimming 0.734\n",
      "mermaid 0.734\n",
      "break 0.734\n",
      "appear 0.734\n",
      "cab 0.733\n",
      "repetative 0.733\n",
      "blade 0.714\n",
      "digimon 0.733\n",
      "halo 0.677\n",
      "armband 0.733\n",
      "bairn 0.733\n",
      "teat 0.733\n",
      "saxophone 0.733\n",
      "chuckle 0.733\n",
      "suitable 0.685\n",
      "fireplace 0.732\n",
      "haha 0.732\n",
      "bruder 0.732\n",
      "promptly 0.68\n",
      "sophisticated 0.732\n",
      "hub 0.732\n",
      "wasnt 0.732\n",
      "extension 0.731\n",
      "velvety 0.731\n",
      "im 0.718\n",
      "repeat 0.731\n",
      "railway 0.668\n",
      "accepted 0.731\n",
      "pic 0.731\n",
      "satisfy 0.731\n",
      "stun 0.713\n",
      "rating 0.731\n",
      "eta 0.73\n",
      "smiler 0.73\n",
      "esl 0.73\n",
      "make 0.73\n",
      "bee 0.73\n",
      "table 0.73\n",
      "decoration 0.69\n",
      "soppy 0.73\n",
      "flower 0.73\n",
      "flag 0.73\n",
      "chatacters 0.73\n",
      "droids 0.73\n",
      "wee 0.716\n",
      "kitchen 0.729\n",
      "ranger 0.729\n",
      "operate 0.729\n",
      "pudsey 0.729\n",
      "embroidery 0.729\n",
      "prescent 0.729\n",
      "colourful 0.722\n",
      "swift 0.677\n",
      "sort 0.729\n",
      "dealer 0.728\n",
      "prompt 0.666\n",
      "choc 0.728\n",
      "budkins 0.728\n",
      "shamrock 0.727\n",
      "yellow 0.727\n",
      "bust 0.727\n",
      "het 0.727\n",
      "full 0.727\n",
      "shiny 0.726\n",
      "inner 0.726\n",
      "strong 0.726\n",
      "undamaged 0.726\n",
      "venue 0.726\n",
      "online 0.726\n",
      "crisp 0.726\n",
      "munchkin 0.688\n",
      "smarten 0.726\n",
      "jokey 0.726\n",
      "seller 0.68\n",
      "arnold 0.726\n",
      "exactely 0.726\n",
      "marker 0.725\n",
      "train 0.678\n",
      "evaluate 0.698\n",
      "firefly 0.725\n",
      "instruction 0.705\n",
      "camera 0.725\n",
      "arthur 0.725\n",
      "rub 0.725\n",
      "antique 0.725\n",
      "business 0.725\n",
      "bannerthank 0.724\n",
      "rex 0.724\n",
      "communication 0.696\n",
      "tattoo 0.724\n",
      "presebt 0.724\n",
      "pony 0.689\n",
      "thx 0.724\n",
      "fastener 0.723\n",
      "another 0.723\n",
      "hulk 0.723\n",
      "playground 0.723\n",
      "adult 0.723\n",
      "quickly 0.722\n",
      "kentrosaurus 0.722\n",
      "htis 0.722\n",
      "noisier 0.722\n",
      "jacket 0.722\n",
      "anger 0.722\n",
      "gadget 0.714\n",
      "decorate 0.679\n",
      "smell 0.722\n",
      "fllters 0.721\n",
      "everthig 0.721\n",
      "jigstrain 0.721\n",
      "tamiya 0.712\n",
      "nrrr 0.721\n",
      "lamp 0.721\n",
      "chelsea 0.721\n",
      "imitator 0.72\n",
      "rob 0.72\n",
      "goid 0.72\n",
      "solder 0.72\n",
      "rag 0.707\n",
      "rares 0.72\n",
      "timescale 0.72\n",
      "everythong 0.719\n",
      "racer 0.719\n",
      "dexter 0.719\n",
      "avid 0.719\n",
      "math 0.719\n",
      "excellently 0.719\n",
      "perform 0.718\n",
      "outside 0.718\n",
      "hp 0.697\n",
      "late 0.715\n",
      "candle 0.669\n",
      "sheet 0.718\n",
      "step 0.718\n",
      "stuffed 0.718\n",
      "locate 0.718\n",
      "role 0.718\n",
      "english 0.718\n",
      "algae 0.718\n",
      "carful 0.718\n",
      "ontime 0.709\n",
      "driver 0.717\n",
      "assmble 0.717\n",
      "tattoosrecommend 0.717\n",
      "long 0.67\n",
      "support 0.717\n",
      "collector 0.678\n",
      "july 0.717\n",
      "u 0.717\n",
      "shopkins 0.717\n",
      "forward 0.716\n",
      "brother 0.716\n",
      "european 0.716\n",
      "try 0.716\n",
      "sure 0.716\n",
      "doomwheel 0.715\n",
      "true 0.697\n",
      "display 0.715\n",
      "blend 0.715\n",
      "ford 0.715\n",
      "f 0.715\n",
      "lil 0.714\n",
      "promply 0.714\n",
      "separate 0.714\n",
      "label 0.714\n",
      "sound 0.714\n",
      "instant 0.714\n",
      "shame 0.714\n",
      "stand 0.714\n",
      "powell 0.714\n",
      "brio 0.713\n",
      "atxmas 0.713\n",
      "brand 0.713\n",
      "pokémon 0.713\n",
      "fawkes 0.713\n",
      "superbly 0.713\n",
      "presant 0.713\n",
      "plane 0.713\n",
      "japanese 0.712\n",
      "hee 0.712\n",
      "simba 0.712\n",
      "enjoyes 0.712\n",
      "agnes 0.712\n",
      "sevice 0.707\n",
      "age 0.712\n",
      "group 0.696\n",
      "trouser 0.712\n",
      "petal 0.712\n",
      "elder 0.711\n",
      "odour 0.711\n",
      "nabbit 0.711\n",
      "cap 0.711\n",
      "trade 0.711\n",
      "millie 0.711\n",
      "han 0.711\n",
      "guess 0.711\n",
      "introduction 0.71\n",
      "glue 0.71\n",
      "anyway 0.71\n",
      "simple 0.71\n",
      "check 0.71\n",
      "pizza 0.71\n",
      "reccomment 0.71\n",
      "go 0.71\n",
      "big 0.672\n",
      "poly 0.71\n",
      "invitation 0.693\n",
      "adhd 0.71\n",
      "rat 0.71\n",
      "exspect 0.71\n",
      "rn 0.709\n",
      "absoluty 0.709\n",
      "outright 0.709\n",
      "dad 0.686\n",
      "building 0.709\n",
      "domino 0.709\n",
      "smurfs 0.709\n",
      "impossible 0.709\n",
      "berlin 0.709\n",
      "yesit 0.709\n",
      "reminisce 0.709\n",
      "book 0.682\n",
      "kept 0.69\n",
      "entertainment 0.678\n",
      "learn 0.679\n",
      "egg 0.708\n",
      "sad 0.708\n",
      "tt 0.708\n",
      "hexacopter 0.708\n",
      "quolity 0.707\n",
      "itemd 0.707\n",
      "efficiant 0.707\n",
      "bestens 0.707\n",
      "alles 0.707\n",
      "trouble 0.707\n",
      "scout 0.707\n",
      "anticipate 0.707\n",
      "oh 0.706\n",
      "colletor 0.706\n",
      "hen 0.706\n",
      "fifty 0.706\n",
      "rose 0.706\n",
      "fa 0.706\n",
      "effect 0.706\n",
      "board 0.694\n",
      "pocket 0.706\n",
      "unsealefd 0.706\n",
      "deliver 0.669\n",
      "grandaughers 0.705\n",
      "timing 0.705\n",
      "figuires 0.705\n",
      "bulldog 0.705\n",
      "pressents 0.705\n",
      "clone 0.705\n",
      "sparkly 0.705\n",
      "delicate 0.689\n",
      "decider 0.705\n",
      "ninjago 0.704\n",
      "mtg 0.704\n",
      "dollhouse 0.704\n",
      "bag 0.704\n",
      "barbie 0.704\n",
      "t 0.704\n",
      "spiky 0.704\n",
      "overjoyed 0.704\n",
      "dus 0.703\n",
      "fin 0.703\n",
      "share 0.703\n",
      "short 0.703\n",
      "practical 0.703\n",
      "nursery 0.703\n",
      "barber 0.703\n",
      "link 0.703\n",
      "nonestop 0.703\n",
      "graphical 0.703\n",
      "bumblebee 0.703\n",
      "uno 0.703\n",
      "tradionalist 0.703\n",
      "wear 0.703\n",
      "customer 0.675\n",
      "thoughgood 0.702\n",
      "optimal 0.702\n",
      "different 0.702\n",
      "wel 0.702\n",
      "horwheel 0.702\n",
      "unidentficable 0.702\n",
      "squirrel 0.669\n",
      "earlier 0.701\n",
      "gie 0.701\n",
      "radiant 0.701\n",
      "gruffalo 0.688\n",
      "hitch 0.674\n",
      "return 0.701\n",
      "woul 0.701\n",
      "cutest 0.7\n",
      "scale 0.7\n",
      "tachnic 0.7\n",
      "gold 0.7\n",
      "chick 0.7\n",
      "draft 0.7\n",
      "packet 0.7\n",
      "technical 0.7\n",
      "competitively 0.694\n",
      "piece 0.7\n",
      "unique 0.677\n",
      "operates 0.699\n",
      "cheer 0.688\n",
      "nicer 0.699\n",
      "control 0.699\n",
      "firestorm 0.699\n",
      "settee 0.699\n",
      "receiver 0.681\n",
      "spelling 0.698\n",
      "lid 0.698\n",
      "wuick 0.698\n",
      "avride 0.698\n",
      "plaster 0.698\n",
      "gamehours 0.698\n",
      "funthanks 0.698\n",
      "excessive 0.697\n",
      "ghreat 0.697\n",
      "chessex 0.697\n",
      "choo 0.697\n",
      "article 0.697\n",
      "colar 0.697\n",
      "dicey 0.697\n",
      "alien 0.697\n",
      "purple 0.684\n",
      "lover 0.673\n",
      "slate 0.696\n",
      "alwyas 0.696\n",
      "track 0.669\n",
      "amendment 0.696\n",
      "papo 0.696\n",
      "filler 0.696\n",
      "ease 0.696\n",
      "weekend 0.683\n",
      "lift 0.696\n",
      "stress 0.696\n",
      "chart 0.695\n",
      "expectation 0.695\n",
      "occasion 0.695\n",
      "ran 0.695\n",
      "ninjagos 0.695\n",
      "gonna 0.695\n",
      "doom 0.694\n",
      "mate 0.694\n",
      "fry 0.694\n",
      "lead 0.694\n",
      "storm 0.694\n",
      "loom 0.671\n",
      "wand 0.694\n",
      "australian 0.694\n",
      "crease 0.694\n",
      "rip 0.694\n",
      "lilabell 0.694\n",
      "motorhome 0.694\n",
      "sexy 0.694\n",
      "sphero 0.694\n",
      "ark 0.693\n",
      "sonic 0.693\n",
      "pearlized 0.693\n",
      "an 0.693\n",
      "tesla 0.693\n",
      "granddaaughter 0.693\n",
      "monarch 0.693\n",
      "runner 0.693\n",
      "crayon 0.693\n",
      "minatures 0.693\n",
      "kratts 0.693\n",
      "mention 0.693\n",
      "project 0.667\n",
      "irritate 0.693\n",
      "rock 0.693\n",
      "deco 0.693\n",
      "suggests 0.692\n",
      "would 0.68\n",
      "whocollects 0.692\n",
      "receive 0.668\n",
      "cord 0.692\n",
      "buiding 0.692\n",
      "profesional 0.691\n",
      "paddle 0.691\n",
      "hat 0.691\n",
      "powerful 0.691\n",
      "instructionsds 0.691\n",
      "hoursf 0.691\n",
      "draughtier 0.691\n",
      "claus 0.691\n",
      "muppet 0.691\n",
      "stag 0.69\n",
      "cloak 0.669\n",
      "britain 0.69\n",
      "atmosphere 0.69\n",
      "act 0.69\n",
      "wagaming 0.69\n",
      "goo 0.69\n",
      "alaya 0.69\n",
      "blunt 0.689\n",
      "discreet 0.689\n",
      "side 0.689\n",
      "absoute 0.689\n",
      "filter 0.689\n",
      "schleich 0.678\n",
      "smash 0.689\n",
      "scotsman 0.689\n",
      "drive 0.689\n",
      "caravan 0.674\n",
      "wash 0.689\n",
      "sash 0.689\n",
      "thoroughly 0.689\n",
      "alaway 0.689\n",
      "terrific 0.688\n",
      "neede 0.688\n",
      "shh 0.688\n",
      "organize 0.688\n",
      "partyeveryone 0.688\n",
      "advert 0.688\n",
      "deatil 0.688\n",
      "cottage 0.688\n",
      "yo 0.688\n",
      "fulfils 0.688\n",
      "competitive 0.687\n",
      "recieved 0.687\n",
      "skin 0.687\n",
      "hatch 0.687\n",
      "communion 0.687\n",
      "taller 0.687\n",
      "cooperation 0.686\n",
      "envelope 0.686\n",
      "matchstick 0.686\n",
      "question 0.686\n",
      "longer 0.686\n",
      "november 0.686\n",
      "smooth 0.668\n",
      "gorilla 0.686\n",
      "trainset 0.686\n",
      "real 0.667\n",
      "metallic 0.685\n",
      "bumped 0.685\n",
      "league 0.685\n",
      "hesitate 0.685\n",
      "door 0.67\n",
      "intricate 0.685\n",
      "tough 0.685\n",
      "gardening 0.685\n",
      "feel 0.685\n",
      "companion 0.685\n",
      "backyard 0.685\n",
      "swim 0.685\n",
      "pinkish 0.684\n",
      "expecially 0.684\n",
      "synchronization 0.684\n",
      "predator 0.684\n",
      "gransons 0.684\n",
      "slept 0.684\n",
      "asexpected 0.684\n",
      "teach 0.684\n",
      "information 0.684\n",
      "playdoh 0.684\n",
      "hawkeye 0.683\n",
      "reminds 0.683\n",
      "well 0.683\n",
      "penny 0.671\n",
      "stockvery 0.683\n",
      "flareon 0.683\n",
      "tassel 0.683\n",
      "lamppost 0.683\n",
      "cosy 0.683\n",
      "hope 0.68\n",
      "ad 0.682\n",
      "childrens 0.682\n",
      "bananagrams 0.682\n",
      "engine 0.682\n",
      "boomerang 0.682\n",
      "knocker 0.682\n",
      "inquisitive 0.681\n",
      "shade 0.681\n",
      "motor 0.681\n",
      "abundant 0.681\n",
      "jolly 0.681\n",
      "test 0.681\n",
      "tape 0.681\n",
      "dcc 0.681\n",
      "bookbug 0.681\n",
      "hospital 0.681\n",
      "duper 0.68\n",
      "happ 0.68\n",
      "bailing 0.68\n",
      "hopefully 0.68\n",
      "nanny 0.68\n",
      "search 0.68\n",
      "site 0.68\n",
      "picked 0.68\n",
      "bonneville 0.679\n",
      "exit 0.679\n",
      "consume 0.679\n",
      "lipo 0.679\n",
      "clap 0.679\n",
      "outrageously 0.679\n",
      "transform 0.669\n",
      "painting 0.679\n",
      "bionicles 0.679\n",
      "pam 0.679\n",
      "name 0.679\n",
      "mislead 0.679\n",
      "someone 0.679\n",
      "magical 0.678\n",
      "cake 0.67\n",
      "strip 0.678\n",
      "hook 0.678\n",
      "novelty 0.678\n",
      "nerdy 0.678\n",
      "arrive 0.678\n",
      "come 0.677\n",
      "contentedly 0.677\n",
      "emu 0.677\n",
      "room 0.677\n",
      "harry 0.677\n",
      "criminal 0.677\n",
      "crystal 0.677\n",
      "ham 0.677\n",
      "brut 0.677\n",
      "turtle 0.676\n",
      "lightsaber 0.676\n",
      "troll 0.676\n",
      "snowman 0.676\n",
      "drone 0.676\n",
      "groovy 0.676\n",
      "baragin 0.676\n",
      "earringswill 0.676\n",
      "letter 0.676\n",
      "wheelbarrow 0.676\n",
      "finite 0.675\n",
      "artefact 0.675\n",
      "update 0.675\n",
      "chalk 0.675\n",
      "hornby 0.675\n",
      "turn 0.675\n",
      "easter 0.675\n",
      "pot 0.675\n",
      "perfection 0.675\n",
      "noisy 0.675\n",
      "lap 0.675\n",
      "kink 0.675\n",
      "event 0.675\n",
      "fillerssearched 0.675\n",
      "people 0.675\n",
      "limousine 0.674\n",
      "decieving 0.674\n",
      "nighmare 0.674\n",
      "delivery 0.674\n",
      "edition 0.674\n",
      "style 0.674\n",
      "element 0.673\n",
      "tad 0.673\n",
      "qualirty 0.673\n",
      "comfy 0.673\n",
      "boo 0.673\n",
      "beados 0.673\n",
      "fitures 0.673\n",
      "log 0.673\n",
      "hung 0.672\n",
      "rg 0.672\n",
      "boxed 0.672\n",
      "thick 0.672\n",
      "power 0.672\n",
      "completely 0.672\n",
      "built 0.672\n",
      "poppet 0.672\n",
      "entry 0.672\n",
      "trinket 0.672\n",
      "ted 0.672\n",
      "bow 0.672\n",
      "metterial 0.672\n",
      "eve 0.671\n",
      "beowulf 0.671\n",
      "advertising 0.671\n",
      "ever 0.671\n",
      "day 0.671\n",
      "pleasure 0.671\n",
      "desribed 0.671\n",
      "th 0.671\n",
      "prezzie 0.671\n",
      "gorgon 0.671\n",
      "tangle 0.67\n",
      "complains 0.67\n",
      "weird 0.67\n",
      "quiet 0.666\n",
      "pathfinder 0.67\n",
      "help 0.67\n",
      "happier 0.67\n",
      "postage 0.67\n",
      "apparently 0.67\n",
      "shiver 0.67\n",
      "earties 0.67\n",
      "pricing 0.67\n",
      "curvy 0.67\n",
      "rate 0.669\n",
      "possibility 0.669\n",
      "exam 0.669\n",
      "hellraiser 0.669\n",
      "grateful 0.669\n",
      "anglia 0.669\n",
      "fascinate 0.668\n",
      "cluedo 0.668\n",
      "brillian 0.668\n",
      "later 0.668\n",
      "greece 0.668\n",
      "stroppy 0.668\n",
      "toothless 0.668\n",
      "art 0.668\n",
      "innotab 0.668\n",
      "veryp 0.668\n",
      "lesed 0.668\n",
      "shouldnt 0.668\n",
      "heron 0.668\n",
      "customise 0.667\n",
      "fluffy 0.667\n",
      "gundam 0.667\n",
      "others 0.667\n",
      "amuse 0.667\n",
      "shoebox 0.667\n",
      "specificdates 0.667\n",
      "pig 0.667\n",
      "particularly 0.667\n",
      "outcome 0.667\n",
      "growl 0.667\n",
      "obb 0.667\n",
      "computer 0.667\n",
      "naff 0.667\n",
      "rocket 0.666\n",
      "breyer 0.666\n",
      "center 0.666\n",
      "lightning 0.666\n",
      "genuine 0.666\n",
      "visit 0.666\n",
      "xg 0.666\n",
      "trash 0.666\n",
      "snow 0.666\n",
      "told 0.666\n",
      "shop 0.666\n",
      "spare 0.666\n",
      "breathing 0.666\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# To sort the tfidf values using built-in\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=5000): #, target):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    #rating = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "        #rating.append(target[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(X.tocoo())\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items) #, Y_train)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11807171219876156, 0.17465587001474128, 0.17174818563676691, 0.11722807360446065, 0.0974028300940781, 0.10655927426431895, 0.12178642445641077, 0.1824196457011426, 0.43771350068719844, 0.12722658719101473, 0.16240289035219083, 0.10745160439139594, 0.12861782305799102, 0.08905604926418609, 0.16913311734089206, 0.10910773584075532, 0.07719419864228257, 0.07993241741049649, 0.15609147942375082, 0.15662717442104432, 0.11558981608336905, 0.19028194389067282, 0.1038494746711616, 0.13723639225147166, 0.2279161760187289, 0.15087922631683842, 0.20882570403269532, 0.15297982191256848, 0.144542637939474, 0.15762493713144543, 0.23539876075319643, 0.1730297211735743, 0.16717328528027378, 0.2279161760187289, 0.13301924424264508, 0.14781570019207643], [0.15088596469687268, 0.127433576147734, 0.15775583888504957, 0.1524231328788433, 0.1364079560534853, 0.2703000437870495, 0.29730543378069335, 0.17750815997098965, 0.25612495193047324, 0.26554499195150055, 0.23394233449823, 0.08468964561573364, 0.2121680192315403, 0.32557005078645174, 0.25779275151483033, 0.1728075596557334, 0.2295281345090719, 0.2288501560476497, 0.14361951033517892, 0.10487321750836963, 0.14567526531169148, 0.286926349315092, 0.12542243405367265], [0.2944670423847172, 0.11797346196966751, 0.1616959625293208, 0.3332060660097004, 0.24792663932717787, 0.19252023783650615, 0.21902469144343678, 0.24330760845041813, 0.30720224537355556, 0.23592012607734406, 0.2623530701618834, 0.09439042597573731, 0.34136970252178633, 0.11994894008847035, 0.18590479520160053, 0.1980357669921575, 0.1434330732456503, 0.18059687276388156, 0.15938435490883626, 0.21315686100093448], [0.36312625144518423, 0.2560278405134285, 0.26478935065490355, 0.14214951679933796, 0.15796007466913073, 0.28385347462842225, 0.16581781246847482, 0.20574509842775035, 0.2170120335999229, 0.322577006258326, 0.23945358254658397, 0.3115058994252025, 0.4223090435184586, 0.230324538070863], [0.22072231276238724, 0.15616199827509672, 0.5884292000636829, 0.42917561454058045, 0.23892050548591506, 0.12904136114523676, 0.19378030202783067, 0.5340121469197334], [0.3657847815806802, 0.3473346113345854, 0.14802632161734705, 0.3465940050530184, 0.5952135474767575, 0.3740858952012277, 0.23212321029325056, 0.23541528467276238], [0.16989625143384615, 0.24184639095897933, 0.3297956815060709, 0.404934704155299, 0.421813348784056, 0.5743723421053067, 0.2684113857731421, 0.24501738746227145], [0.5744269102997266, 0.18742787443844033, 0.16389573563193044, 0.30998277456494494, 0.4439187453480855, 0.42231246398525757, 0.3695155128177531], [0.28229620393154153, 0.5846083172674199, 0.7606194637449117], [0.8365703941801363, 0.5478594487469309]]\n",
      "[4, 5, 5, 5, 5, 5, 5, 5, 4, 5]\n",
      "28167 28167\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Put individual tf-idf values into list format based on the rows (review index)\n",
    "previous_row = 0\n",
    "temp = []\n",
    "tfidf_val_1 = []\n",
    "rating_list_1 = []\n",
    "for row, col in zip(rows, cols):\n",
    "    if row == previous_row:\n",
    "        temp.append(X[row,col])\n",
    "        \n",
    "    else:\n",
    "        tfidf_val_1.append(temp)\n",
    "        try:\n",
    "            temp_rating = int(Y_train[corpus_index[previous_row]])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        rating_list_1.append(temp_rating)\n",
    "        temp = []\n",
    "        previous_row = row\n",
    "        temp.append(X[row,col])\n",
    "        \n",
    "tfidf_val_1.append(temp)\n",
    "try:\n",
    "    temp_rating = int(Y_train[corpus_index[previous_row]])\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "rating_list_1.append(temp_rating)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Convert the tfidf result into individual record\n",
    "tfidf_val_1 = []\n",
    "temp = []\n",
    "rating_list_1 = []\n",
    "for row, col in zip(rows, cols):\n",
    "    #print((feature_names[col], corpus_index[row]), X_1[row, col])\n",
    "    val = X[row, col]\n",
    "    tfidf_val_1.append(val)\n",
    "    \n",
    "    try:\n",
    "        temp = int(Y_train[corpus_index[row]])\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    rating_list_1.append(temp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tfidf  rating\n",
      "0  0.118072       4\n",
      "1  0.174656       4\n",
      "2  0.171748       4\n",
      "3  0.117228       4\n",
      "4  0.097403       4\n",
      "5  0.106559       4\n",
      "6  0.121786       4\n",
      "7  0.182420       4\n",
      "8  0.437714       4\n",
      "9  0.127227       4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_tfidf = pd.DataFrame({'tfidf': tfidf_val_1})\n",
    "df_rating = pd.DataFrame({'rating': rating_list_1})\n",
    "combo = pd.concat([df_tfidf, df_rating], axis= 1)\n",
    "print(combo.head(10))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28212, 18905) 28212\n",
      "Oversampling:  60.15819501876831\n",
      "100937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "After tfidf and rating for each review, do kfold, undersample/ oversample\n",
    "and pass into model\n",
    "\"\"\"\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# MinMaxScaler\n",
    "# scaler1 = MinMaxScaler(feature_range=(0,1))\n",
    "# X = scaler1.fit_transform(X)\n",
    "\n",
    "# scaler2 = MinMaxScaler(feature_range=(1,5))\n",
    "# Y = scaler2.fit_transform(Y)\n",
    "Y = Y_train.ravel()   # flattened of array as MLP expects 1-d array\n",
    "print(X.shape, len(Y))\n",
    "\n",
    "# k fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # Do oversampling for training data\n",
    "    start = time.time()\n",
    "    ada = ADASYN(random_state = 4, sampling_strategy='auto')\n",
    "    #X_res, y_res = ada.fit_resample(X_train, Y_train)\n",
    "    X_res, y_res = ada.fit_resample(X, Y)\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    print(\"Oversampling: \",elapsed)\n",
    "    print(len(y_res))\n",
    "    \n",
    "    # Do Linear SVC\n",
    "    classifier = OneVsRestClassifier(svm.LinearSVC())   # multi_class = \"crammer_singer\" for multi-class labels\n",
    "    classifier.fit(X_res, y_res) \n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(Y_test, predictions))\n",
    "    print(classification_report(Y_test,predictions))\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "# Do undersampling for training data (RandomUnderSampler)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X_train, Y_train)\n",
    "\n",
    " # Do undersampling (NearMiss)\n",
    "nr = NearMiss()\n",
    "X_res, y_res = nr.fit_resample(X_train, Y_train)\n",
    "print(len(y_res))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "    #     start1 = time.time()\n",
    "    \"\"\"\n",
    "    \n",
    "    mlp = MLPClassifier(max_iter=100)\n",
    "    parameter_space = {\n",
    "'hidden_layer_sizes': [(5,2), (10, 3), (7,4)],\n",
    "'activation': ['tanh', 'relu'],\n",
    "'solver': ['sgd', 'adam'],\n",
    "'alpha': [0.0001, 0.05],\n",
    "'learning_rate': ['constant','adaptive'],}\n",
    "clf = GridSearchCV(mlp, parameter_space, cv=10)\n",
    "clf.fit(X_res, y_res)\n",
    "\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "#     # Do MLP\n",
    "#     mlp = MLPClassifier(solver='sgd', hidden_layer_sizes= (5,5), random_state=1)\n",
    "#     mlp.fit(X_res, y_res)\n",
    "#     predictions = mlp.predict(X_test)\n",
    "#     for i in range(len(mlp.coefs_)):\n",
    "#         number_neurons_in_layer = mlp.coefs_[i].shape[1]\n",
    "#         for j in range(number_neurons_in_layer):\n",
    "#             weights = mlp.coefs_[i][:,j]\n",
    "#             print(i, j, weights, end=\", \")\n",
    "#             print()\n",
    "#         print()\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Do SVC(rbf)\n",
    "#     rbf_svc = SVC(kernel='rbf')\n",
    "#     rbf_svc.fit(X_res, y_res)\n",
    "#     predictions = rbf_svc.predict(X_test)\n",
    "    \n",
    "    # Do OnevsRest\n",
    "#     classifier = OneVsRestClassifier(SVC(kernel='rbf'))\n",
    "#     classifier.fit(X_res, y_res)\n",
    "#     predictions = classifier.predict(X_test)\n",
    "    \n",
    "#     \n",
    "#     done1 = time.time()\n",
    "#     elapsed1 = done1 - start1\n",
    "#     print(\"Linear SVM (rbf): \", elapsed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
